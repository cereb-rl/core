{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePolicy:\n",
    "    \"\"\" Base Policy Class\"\"\"\n",
    "\n",
    "    def __init__(self, action_space):\n",
    "        \"\"\"\n",
    "\n",
    "        :param action_space: OpenAI Gym Space object\n",
    "        \"\"\"\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "\n",
    "        :param state: current environment state\n",
    "        :return: action to take based on the policy\n",
    "        \"\"\"\n",
    "        return self.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAgent(object):\n",
    "    \"\"\" Abstract Agent class. \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: Space, action_space: Space, name=\"BaseAgent\", params={'gamma': 0.95},\n",
    "                 specs=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param observation_space: Environment's observation space\n",
    "        :param action_space: Environment's action space\n",
    "        :param name: Agent's Name\n",
    "        :param params: Hyper-parameters etc\n",
    "        :param specs: Specifies space types agent is compactible with (eg Discrete for RMax)\n",
    "        \"\"\"\n",
    "\n",
    "    def learn(self, state, reward=None, done=False):\n",
    "        \"\"\"\n",
    "        Returns an action during agent's training\n",
    "\n",
    "        :param state: new environment state\n",
    "        :param reward: reward due to arriving in state\n",
    "        :param done: boolean indicating whether the episode is complete\n",
    "        :return action: an action to take on the environment\n",
    "        \"\"\"\n",
    "    \n",
    "    def predict(self, state):\n",
    "        \"\"\"\n",
    "        Returns an action during agent's training\n",
    "\n",
    "        :param state: new environment state\n",
    "        :return action: agent's optimal action\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "    def start_of_episode(self, state):\n",
    "        \"\"\"\n",
    "\n",
    "        :param state: environment start state\n",
    "        :return: action to take\n",
    "        \"\"\"\n",
    "\n",
    "    def end_of_episode(self):\n",
    "        \"\"\"\n",
    "        ends episode\n",
    "        \"\"\"\n",
    "\n",
    "    def _stepwise_update(self, state, reward):\n",
    "        \"\"\"\n",
    "        agent implements this\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _episodic_update(self):\n",
    "        \"\"\"\n",
    "        agent implements this\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "RMaxAgentClass.py: Class for an RMaxAgent from [Strehl, Li and Littman 2009].\n",
    "\n",
    "Notes:\n",
    "    - Assumes WLOG reward function codomain is [0,1] (so RMAX is 1.0)\n",
    "'''\n",
    "\n",
    "# Python imports.\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "# Local classes.\n",
    "from core.agents.base import BaseAgent\n",
    "from core.agents.models import RMaxModel\n",
    "from core.agents.policies import ExploreLeastKnown, DiscreteTabularPolicy\n",
    "from core.utils import constants, specs\n",
    "\n",
    "\n",
    "RMAX_DEFAULTS = {\n",
    "    'epsilon': 0,  # There's no exploration in R-Max\n",
    "    'gamma': 0.95,  # discount factor\n",
    "    'known_threshold': 5,  # number of occurrences of (state, action) pairs before it is marked as known\n",
    "    'max_reward': 1,  # maximum reward\n",
    "    'epsilon_one': 0.99,  #  precision parameter for policy iterations\n",
    "    'max_stepwise_backups': 20,  # maximum number of backups per experience/transition during training\n",
    "    'max_episodic_backups': 0,  # maximum number of backups at the end of an episode\n",
    "}\n",
    "\n",
    "RMAX_SPEC = specs.AgentSpec(\n",
    "    observation_space=constants.SpaceTypes.DISCRETE,\n",
    "    action_space=constants.SpaceTypes.DISCRETE\n",
    ")\n",
    "\n",
    "class RMaxAgent(BaseAgent):\n",
    "    '''\n",
    "    Implementation for an R-Max Agent [Strehl, Li and Littman 2009]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, observation_space, action_space, name=\"RMax Agent\", parameters={}, starting_policy=None):\n",
    "        BaseAgent.__init__(self, observation_space, action_space, name, params=dict(RMAX_DEFAULTS, **parameters), specs=RMAX_SPEC)\n",
    "\n",
    "        # Policy Setup\n",
    "        if starting_policy:\n",
    "            self.predict_policy = starting_policy\n",
    "        else:\n",
    "            self.predict_policy = DiscreteTabularPolicy(self.observation_space, self.action_space, default_value=1/(1-self.gamma))\n",
    "        self.backup_lim = int(np.log(1/(self.params['epsilon_one'] * (1 - self.gamma))) / (1 - self.gamma))\n",
    "        self.policy_iterations = 0\n",
    "\n",
    "        # Model Setup\n",
    "        self.model = RMaxModel(observation_space, action_space, default_reward=self.params['max_reward'], limit=self.params['known_threshold'])\n",
    "\n",
    "        self.learn_policy = ExploreLeastKnown(\n",
    "                action_space=self.action_space,\n",
    "                policy=self.predict_policy,\n",
    "                model=self.model\n",
    "            )\n",
    "\n",
    "    def _stepwise_update(self, state, reward):\n",
    "        if not self.model.is_known(self.prev_state, self.prev_action):\n",
    "            self.model.update(self.prev_state, self.prev_action, reward, state)\n",
    "            if self.model.is_known_state(self.prev_state):\n",
    "                self.vectorized_iterate_policy(num_steps=min(self.backup_lim, self.params['max_stepwise_backups']))\n",
    "\n",
    "    def _episodic_update(self):\n",
    "        self.vectorized_iterate_policy(num_steps=self.params['max_episodic_backups'])\n",
    "\n",
    "    def _vectorized_iterate_policy(self, num_steps):\n",
    "        for _ in range(num_steps):\n",
    "            assert (self.model.known_rewards < 1).any()\n",
    "            self.predict_policy.q_table = self.model.known_rewards + self.gamma*np.dot(self.model.known_transitions, self.predict_policy.get_max_q_values())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}